{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score, balanced_accuracy_score\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sick (/home/imger/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "Found cached dataset multi_nli (/home/imger/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "Found cached dataset multi_nli (/home/imger/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "Found cached dataset esnli (/home/imger/.cache/huggingface/datasets/esnli/plain_text/0.0.2/262495ebbd9e71ec9b0c37a93e378f1b353dc28bb904305e011506792a02996b)\n"
     ]
    }
   ],
   "source": [
    "sick = load_dataset(\"sick\", split=\"validation\")\n",
    "multinli_matched = load_dataset(\"multi_nli\", split=\"validation_matched\")\n",
    "multinli_mismatched = load_dataset(\"multi_nli\", split=\"validation_mismatched\")\n",
    "esnli = load_dataset(\"../datasets/esnli.py\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = tokenizer(\"<mask>\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "# as described in outline\n",
    "# ENTAILMENT_WORDS = [\"as\", \"because\", \"so\"]\n",
    "# NEUTRAL_WORDS = [\"and\", \"also\", \"or\"]\n",
    "# CONTRADICTION_WORDS = [\"but\", \"although\", \"still\"]\n",
    "\n",
    "# improved word choice:\n",
    "# count most probable 5 words for sample and aggregate per label\n",
    "# subtract amount of all other labels\n",
    "# take words with biggest count for each label\n",
    "# result => Words that are common for that label but uncommon for others\n",
    "# done for a random subset of the training set of multinli\n",
    "ENTAILMENT_WORDS = [\"Also\", \"Yes\", \"More\", \"Certainly\", \"yes\", \"by\", \"Specifically\", \"Indeed\", \"Yeah\"]\n",
    "NEUTRAL_WORDS = [\"Apparently\", \"Perhaps\", \"Clearly\", \"Obviously\", \"Presumably\"]\n",
    "CONTRADICTION_WORDS = [\"Yet\", \"However\", \"but\", \"Unfortunately\", \"Otherwise\", \"Except\", \"no\", \"Nearly\", \"Currently\", \"Sadly\", \"Instead\", \"Not\", \"Previously\", \"Until\"]\n",
    "\n",
    "# simple words (surprisingly good)\n",
    "# ENTAILMENT_WORDS = [\"yes\"]\n",
    "# NEUTRAL_WORDS = [\"maybe\"]\n",
    "# CONTRADICTION_WORDS = [\"no\"]\n",
    "\n",
    "ENTAILMENT_IDS = tokenizer.convert_tokens_to_ids([\"Ġ\"+word for word in ENTAILMENT_WORDS])\n",
    "NEUTRAL_IDS = tokenizer.convert_tokens_to_ids([\"Ġ\"+word for word in NEUTRAL_WORDS])\n",
    "CONTRADICTION_IDS = tokenizer.convert_tokens_to_ids([\"Ġ\"+word for word in NEUTRAL_WORDS])\n",
    "\n",
    "def zero_shot_classify(premise, hypothesis):\n",
    "    hypothesis = hypothesis.split(\" \")\n",
    "    hypothesis[0] = hypothesis[0].lower()\n",
    "    hypothesis = \" \".join(hypothesis)\n",
    "    combined = f\"{premise} <mask> {hypothesis}\"\n",
    "    tokenized, attention_mask = tokenizer(combined, return_tensors=\"pt\", add_special_tokens=True).values()\n",
    "\n",
    "    logits = model(tokenized, attention_mask=attention_mask)[\"logits\"]\n",
    "    mask_idx = tokenized[0].tolist().index(MASK)\n",
    "    entailment_score = max(logits[0, mask_idx, ENTAILMENT_IDS])\n",
    "    neutral_score = max(logits[0, mask_idx, NEUTRAL_IDS])\n",
    "    contradiction_score = max(logits[0, mask_idx, CONTRADICTION_IDS])\n",
    "\n",
    "    max_score = max(entailment_score, neutral_score, contradiction_score)\n",
    "    if entailment_score == max_score:\n",
    "        return 0\n",
    "    if neutral_score == max_score:\n",
    "        return 1\n",
    "    if contradiction_score == max_score:\n",
    "        return 2\n",
    "    assert False, \"This should never be reached\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(dataset, premise_key, hypothesis_key, label_key=\"label\"):\n",
    "    true, predicted = [], []\n",
    "    for d in tqdm(dataset):\n",
    "        true.append(d[label_key])\n",
    "        predicted.append(zero_shot_classify(d[premise_key], d[hypothesis_key]))\n",
    "    print(f\"MCC: {matthews_corrcoef(true, predicted)}\")\n",
    "    print(f\"F1: {f1_score(true, predicted, average='macro')}\")\n",
    "    print(f\"Acc: {accuracy_score(true, predicted)}\")\n",
    "    print(f\"BAcc: {balanced_accuracy_score(true, predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [00:24<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.04600834360286191\n",
      "F1: 0.21167878243349944\n",
      "Acc: 0.33535353535353535\n",
      "BAcc: 0.3571825564708127\n"
     ]
    }
   ],
   "source": [
    "analyze(sick, \"sentence_A\", \"sentence_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9815/9815 [11:26<00:00, 14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.11392030968470503\n",
      "F1: 0.31279251578936584\n",
      "Acc: 0.4044829342842588\n",
      "BAcc: 0.3946197962978763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze(multinli_matched, \"premise\", \"hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9832/9832 [11:01<00:00, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.1283559309201467\n",
      "F1: 0.3214575127463627\n",
      "Acc: 0.41120829943043125\n",
      "BAcc: 0.4040290359228627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze(multinli_mismatched, \"premise\", \"hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9842/9842 [09:17<00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.13421134002556806\n",
      "F1: 0.327133422273503\n",
      "Acc: 0.4106888843730949\n",
      "BAcc: 0.4097624903100461\n"
     ]
    }
   ],
   "source": [
    "analyze(esnli, \"premise\", \"hypothesis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on which tokens are most prevalent for specific labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from itertools import islice\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(premise, hypothesis):\n",
    "    hypothesis = hypothesis.split(\" \")\n",
    "    hypothesis[0] = hypothesis[0].lower()\n",
    "    hypothesis = \" \".join(hypothesis)\n",
    "    combined = f\"{premise} <mask> {hypothesis}\"\n",
    "    tokenized, attention_mask = tokenizer(combined, return_tensors=\"pt\", add_special_tokens=True).values()\n",
    "\n",
    "    logits = model(tokenized, attention_mask=attention_mask)[\"logits\"]\n",
    "    mask_idx = tokenized[0].tolist().index(MASK)\n",
    "    return tokenizer.convert_ids_to_tokens(torch.topk(logits[0, mask_idx], k=3).indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/imger/.cache/huggingface/modules/datasets_modules/datasets/multi_nli/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39 (last modified on Sat Feb 25 17:09:18 2023) since it couldn't be found locally at multi_nli., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset multi_nli (/home/imger/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "multinli_train = load_dataset(\"multi_nli\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [53:20<00:00, 15.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "tokens_by_label = {0: [], 1: [], 2: []}\n",
    "size = 50_000\n",
    "for d in tqdm(islice(multinli_train, size), total=size):\n",
    "    tokens_by_label[d[\"label\"]].extend(predict_mask(d[\"premise\"], d[\"hypothesis\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [('Ġ,', 114), ('ĠAlso', 74), ('ĠYes', 71), ('ĠMore', 31), ('ĠCertainly', 29), ('Ġyes', 26), ('Ġby', 23), ('The', 21), ('ĠBelow', 19), ('ĠSpecifically', 18), ('ĠIndeed', 18), ('ĠHow', 17), ('Ġ:', 17), ('Ġ.', 15), ('ĠYeah', 15)]\n",
      "1 [('ĠApparently', 77), ('ĠPerhaps', 33), ('ĠClearly', 30), ('ĠObviously', 25), ('ĠPresumably', 21), ('ĠPlus', 13), ('ĠAlready', 12), ('ĠAmong', 9), ('ĠTogether', 8), ('ĠMajor', 6), ('Ġdollars', 6), ('ĠContinuous', 5), ('ĠSure', 5), ('ĠDespite', 5), ('Ġensure', 5)]\n",
      "2 [('ĠYet', 1040), ('ĠHowever', 668), ('Ġbut', 542), ('ĠUnfortunately', 373), ('ĠOtherwise', 206), ('ĠExcept', 151), ('ĠAlmost', 143), ('Ġno', 64), ('ĠNearly', 46), ('ĠCurrently', 43), ('ĠSadly', 43), ('ĠInstead', 30), ('ĠNot', 27), ('ĠPreviously', 27), ('ĠUntil', 27)]\n"
     ]
    }
   ],
   "source": [
    "counters = {label : Counter(words) for label, words in tokens_by_label.items()}\n",
    "counters_ = {label : copy.deepcopy(c) for label, c in counters.items()}\n",
    "\n",
    "for label in counters_.keys():\n",
    "    for word in counters_[label].keys():\n",
    "        for other in counters.keys():\n",
    "            if word not in counters[other] or other==label:\n",
    "                continue\n",
    "            counters_[label][word] -= counters[other][word]\n",
    "\n",
    "for label, c in counters_.items():\n",
    "    print(label, c.most_common()[:15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2aa3fb215bca76f53379a5e759b4870fc087d7c7f27d239b8095eb76fe98d86f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
