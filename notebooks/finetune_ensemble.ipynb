{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score, balanced_accuracy_score\n",
    "from transformers import RobertaForSequenceClassification, RobertaModel, RobertaConfig, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaClassificationHead\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/semproj/sem_proj22/proj_05/data/models/roberta-ensemble-finetuned-mnli/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "checkpoint_path = os.path.join(model_path, \"checkpoints\")\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        self.bias_head = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.hypothesis_only = RobertaForSequenceClassification(config)\n",
    "        self.hypothesis_only.eval()\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        token_type_ids = None,\n",
    "        position_ids = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        hypothesis_input_ids = None,\n",
    "        hypothesis_attention_mask = None,\n",
    "        hypothesis_token_type_ids = None,\n",
    "        hypothesis_position_ids = None,\n",
    "        hypothesis_head_mask = None,\n",
    "        hypothesis_inputs_embeds = None,\n",
    "        labels = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        entropy = None\n",
    "        if self.training:\n",
    "            bias_weight = F.softplus(self.bias_head(sequence_output))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                hypothesis_logits = self.hypothesis_only(\n",
    "                    hypothesis_input_ids,\n",
    "                    attention_mask=hypothesis_attention_mask,\n",
    "                    token_type_ids=hypothesis_token_type_ids,\n",
    "                    position_ids=hypothesis_position_ids,\n",
    "                    head_mask=hypothesis_head_mask,\n",
    "                    inputs_embeds=hypothesis_inputs_embeds,\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False,\n",
    "                    return_dict=True,        \n",
    "                ).logits\n",
    "            logits += bias_weight*hypothesis_logits\n",
    "            probs = F.softmax(logits)\n",
    "            entropy = torch.sum(-probs*torch.log(probs))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = nn.MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            \n",
    "            if entropy is not None:\n",
    "                loss += entropy\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_json_file(\"../models/sequence_classification.json\")\n",
    "model = EnsembleForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/students/imgrund/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/students/imgrund/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 415, in load_state_dict\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/torch/serialization.py\", line 795, in load\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/torch/serialization.py\", line 1002, in _legacy_load\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 419, in load_state_dict\n",
      "  File \"/usr/lib/python3.8/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "  File \"/tmp/ipykernel_1488862/3396058473.py\", line 1, in <module>\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2301, in from_pretrained\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 431, in load_state_dict\n",
      "OSError: Unable to load weights from pytorch checkpoint file for '/home/students/imgrund/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin' at '/home/students/imgrund/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1049, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1017, in get_records\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/stack_data/core.py\", line 597, in stack_data\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/stack_data/utils.py\", line 83, in collapse_repeated\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/stack_data/core.py\", line 587, in mapper\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/stack_data/core.py\", line 551, in __init__\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/executing/executing.py\", line 359, in executing\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/executing/executing.py\", line 277, in for_frame\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/executing/executing.py\", line 306, in for_filename\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/executing/executing.py\", line 317, in _for_filename_and_lines\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/executing/executing.py\", line 257, in __init__\n",
      "  File \"/usr/lib/python3.8/ast.py\", line 47, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "state_dict = roberta.state_dict()\n",
    "del state_dict[\"pooler.dense.weight\"]\n",
    "del state_dict[\"pooler.dense.bias\"]\n",
    "model.roberta.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "  File \"/tmp/ipykernel_1488862/4064093420.py\", line 1, in <module>\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2079, in from_pretrained\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 538, in from_pretrained\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 565, in get_config_dict\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 650, in _get_config_dict\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 736, in _dict_from_json_file\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "  File \"/home/students/imgrund/lit/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1003, in get_records\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 979, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 798, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "hypothesis_only = RobertaForSequenceClassification.from_pretrained(\"/mnt/semproj/sem_proj22/proj_05/data/models/roberta-base-finetuned-mnli-hypothesis-only/1337/\")\n",
    "model.hypothesis_only.load_state_dict(hypothesis_only.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, premise_column=\"premise\", hypothesis_column=\"hypothesis\"):\n",
    "    \"\"\"\n",
    "    tokenizes columns with premise and hypothesis of a dataset\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(lambda d: tokenizer(\n",
    "        text=d[premise_column],\n",
    "        text_pair=d[hypothesis_column],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "        ), batched=True)\n",
    "    def tokenize_hypothesis_only(d):\n",
    "        temp = tokenizer(\n",
    "            text=d[premise_column],\n",
    "            text_pair=d[hypothesis_column],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            \"hypothesis_input_ids\": temp[\"input_ids\"],\n",
    "            \"hypothesis_attention_mask\": temp[\"attention_mask\"],\n",
    "            }\n",
    "    dataset = dataset.map(tokenize_hypothesis_only, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"hypothesis_input_ids\", \"hypothesis_attention_mask\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Calculates a few helpful metrics\n",
    "    :param pred: list\n",
    "    \"\"\"\n",
    "    true = pred.label_ids\n",
    "    predicted = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"MCC\": matthews_corrcoef(true, predicted),\n",
    "        \"F1\": f1_score(true, predicted, average='macro'),\n",
    "        \"Acc\": accuracy_score(true, predicted),\n",
    "        \"BAcc\": balanced_accuracy_score(true, predicted),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = load_dataset(\"multi_nli\")\n",
    "mnli_train = preprocess_dataset(mnli[\"train\"])\n",
    "mnli_val = preprocess_dataset(mnli[\"validation_matched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_path,         # output directory\n",
    "    num_train_epochs=3,                 # total number of training epochs\n",
    "    per_device_train_batch_size=8,      # original training has batch_size 16 =>\n",
    "    gradient_accumulation_steps=2,      # 2 accumulation steps, as we can maximally use 8 as batch size\n",
    "    per_device_eval_batch_size=32,      # batch size for evaluation\n",
    "    warmup_steps=0,                     # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.0,                   # strength of weight decay\n",
    "    learning_rate=2e-5,                 # learning rate\n",
    "    logging_dir='./logs',               # directory for storing logs\n",
    "    load_best_model_at_end=True,        # load the best model when finished training\n",
    "    logging_steps=10000,                # log & save weights each logging_steps\n",
    "    save_steps=10000,\n",
    "    evaluation_strategy=\"steps\",        # evaluate each `logging_steps`\n",
    "    log_level=\"info\",                   # log evaluation results\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # the instantiated Transformers model to be trained\n",
    "    args=training_args,                 # training arguments, defined above\n",
    "    train_dataset=mnli_train,           # training dataset\n",
    "    eval_dataset=mnli_val,              # evaluation dataset\n",
    "    compute_metrics=compute_metrics,    # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.evaluate()\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeca8879896bfa2476197eaddb1424585443739a758808f2f78a1d17fd6043ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
