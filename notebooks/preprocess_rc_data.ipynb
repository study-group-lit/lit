{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import DatasetDict, Dataset\n",
    "from multiprocessing import cpu_count\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, pipeline\n",
    "\n",
    "index_mapping = {\n",
    "    \"url\": 0,\n",
    "    \"text\": 1,\n",
    "    \"answer_masked\": 2,\n",
    "    \"correct_entity\": 3,\n",
    "    \"entity_mapping\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['url', 'text', 'answer_masked', 'correct_entity', 'entity_mapping'],\n",
      "        num_rows: 3924\n",
      "    })\n",
      "    training: Dataset({\n",
      "        features: ['url', 'text', 'answer_masked', 'correct_entity', 'entity_mapping'],\n",
      "        num_rows: 380298\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['url', 'text', 'answer_masked', 'correct_entity', 'entity_mapping'],\n",
      "        num_rows: 3198\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "cnn_path = \"../../lit-data/datasets/cnn/questions\"\n",
    "cnn_datasetdict = DatasetDict()\n",
    "splits = [\"training\", \"test\", \"validation\"]\n",
    "\n",
    "for split in splits:\n",
    "    split_path = f\"{cnn_path}/{split}\"\n",
    "    split_list = []\n",
    "    for file_name in os.listdir(split_path):\n",
    "        with open(f\"{split_path}/{file_name}\", \"r\") as f:\n",
    "            parts = f.read().split(\"\\n\\n\")\n",
    "            split_list.append({\n",
    "                \"url\": parts[0],\n",
    "                \"text\": parts[1],\n",
    "                \"answer_masked\": parts[2],\n",
    "                \"correct_entity\": parts[3],\n",
    "                \"entity_mapping\": parts[4]\n",
    "            })\n",
    "    cnn_datasetdict[split] = Dataset.from_list(split_list)\n",
    "\n",
    "print(cnn_datasetdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9f85bda679491da8d8d6b8912ca493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e61fb5f7ea46e4a44bd587f0ed8f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/380298 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab546b3d25e04b9ca0369b6a10b9080f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_datasetdict.save_to_disk(\"../../lit-data/datasets/cnn_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymail_path = \"../../lit-data/datasets/dailymail/questions\"\n",
    "dailymail_datasetdict = DatasetDict()\n",
    "splits = [\"training\", \"test\", \"validation\"]\n",
    "\n",
    "for split in splits:\n",
    "    split_path = f\"{dailymail_path}/{split}\"\n",
    "    split_list = []\n",
    "    for file_name in os.listdir(split_path):\n",
    "        with open(f\"{split_path}/{file_name}\", \"r\") as f:\n",
    "            parts = f.read().split(\"\\n\\n\")\n",
    "            split_list.append({\n",
    "                \"url\": parts[0],\n",
    "                \"text\": parts[1],\n",
    "                \"answer_masked\": parts[2],\n",
    "                \"correct_entity\": parts[3],\n",
    "                \"entity_mapping\": parts[4]\n",
    "            })\n",
    "    dailymail_datasetdict[split] = Dataset.from_list(split_list)\n",
    "\n",
    "print(dailymail_datasetdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymail_datasetdict.save_to_disk(\"../../lit-data/datasets/dailymail_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'could it get any cuter than seal pup kisses ? the USGS and the U.S. Department of Interior this week shared a photo of a Weddell seal nuzzling up to what looked to be its mom in CNN2 , CNN3 . the expression of the mother is priceless . the photo was taken in october by USGS scientist Link . link , a statistician , was helping researchers tag newborn seal pups . he confirmed friday that the adult seal was the baby \\'s mom . it \\'s hard to know what she was thinking when her baby nuzzled up to her in this photo , but Link said the animals flare their noses when disturbed , \" so this Mamma was pretty relaxed , \" Link told CNN friday . \" i have a great shot a few seconds later where Mamma yawned hugely . she looked utterly content , to me . \" the agency \\'s public affairs department had asked scientists for interesting images to post on social media . as the USGS1 caption notes , the Weddell seals of CNN2 have been studied extensively for over 40 years . \" because of its isolation , this population is undisturbed by human activities . the weddell seal population is healthy and stable , and thus gives a good example for studies of animal population dynamics . \" Link said it was a privilege to see the animals up close . \" i was awed , \" he said . \" it \\'s incredible that animals can live and thrive in such harsh conditions . it \\'s hard to describe the remoteness and isolation of the spot -- bitter cold , high winds , no life to be seen except for the seals , an occasional skua or an emperor penguin . \" after a seal pup is born , Link said its mom spends all her time close to her baby in a very small area , with hardly any interaction with other seals . \" Mamma does n\\'t even leave for a swim until the baby is nearly ready to be weaned , and even then does n\\'t get to eat . so while baby puts on a couple of hundred pounds , Mamma loses about twice that much , \" he said . for more information on the seals , visit WeddellSealScience.com .',\n",
       " 'correct_entity': 'Antarctica',\n",
       " 'source': 'CNN',\n",
       " 'answer_entities': 'USGS Weddell',\n",
       " 'answer': 'the USGS has been studying Weddell seals in @placeholder for more than 40 years'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_entity_mapping(mapping_str):\n",
    "    mappings_list = mapping_str.split(\"\\n\")\n",
    "    mappings = dict()\n",
    "    for mapping in mappings_list:\n",
    "        mapping_splits = mapping.split(\":\")\n",
    "        mappings[mapping_splits[0]] = mapping_splits[1]\n",
    "    return mappings\n",
    "\n",
    "def preprocess(record):\n",
    "    entity_mapping = parse_entity_mapping(record[\"entity_mapping\"])\n",
    "\n",
    "    # unmask text\n",
    "    text = record[\"text\"]\n",
    "    for placeholder, entity in entity_mapping.items():\n",
    "        text = re.sub(placeholder, entity, text)\n",
    "    split_text = text.split(\" \")\n",
    "    source = split_text[1]\n",
    "    text = \" \".join(split_text[3:])\n",
    "\n",
    "    # unmask correct entity\n",
    "    correct_entity = entity_mapping[record[\"correct_entity\"]]\n",
    "\n",
    "    # get entites in answer\n",
    "    answer_entites_list = []\n",
    "    answer = record[\"answer_masked\"]\n",
    "    split_answer = answer.split(\" \")\n",
    "    for token in split_answer:\n",
    "        if token.startswith(\"@\"):\n",
    "            try:\n",
    "                answer_entites_list.append(entity_mapping[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    answer_entities = \" \".join(answer_entites_list)\n",
    "\n",
    "    # unmask answer\n",
    "    answer = record[\"answer_masked\"]\n",
    "    for placeholder, entity in entity_mapping.items():\n",
    "        answer = re.sub(placeholder, entity, answer)\n",
    "\n",
    "    return {\"text\": text, \"correct_entity\": correct_entity, \"source\": source, \"answer_entities\": answer_entities, \"answer\": answer }\n",
    "\n",
    "sample = cnn_datasetdict[\"validation\"][0]\n",
    "preprocess(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailymail_datasetdict = DatasetDict.load_from_disk(\"../../lit-data/datasets/dailymail_dataset\")\n",
    "cnn_datasetdict = DatasetDict.load_from_disk(\"../../lit-data/datasets/cnn_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b5d78616e4caa96db4543824c463c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c387d4adb85c4bf28d7174939bb39f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "170301"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = cnn_datasetdict[\"validation\"].select(range(32)).map(preprocess, num_proc=cpu_count())\n",
    "test_set.to_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d480603c53de407ea6d7138f2ff9898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973aef3757fd4156862454519af99609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "181138"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_summary_column(records, model: BartForConditionalGeneration, tokenizer: AutoTokenizer):\n",
    "    texts = records[\"text\"]\n",
    "    inputs = tokenizer(texts, max_length=1024, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=100)\n",
    "    summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return { \"summary\": summaries }\n",
    "\n",
    "with_summaries = test_set.map(add_summary_column, fn_kwargs={\"model\": bart_model, \"tokenizer\": bart_tokenizer}, batched=True, batch_size=32)\n",
    "with_summaries.to_csv(\"./test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lit",
   "language": "python",
   "name": "lit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
