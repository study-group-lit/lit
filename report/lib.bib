@article{ferret,
  title={ferret: a Framework for Benchmarking Explainers on Transformers},
  author={Attanasio, Giuseppe and Pastor, Eliana and Di Bonaventura, Chiara and Nozza, Debora},
  journal={arXiv preprint arXiv:2208.01575},
  year={2022}
}

@inproceedings{snli,
  title = "A large annotated corpus for learning natural language inference",
  author = "Bowman, Samuel R.  and
    Angeli, Gabor  and
    Potts, Christopher  and
    Manning, Christopher D.",
  booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
  month = sep,
  year = "2015",
  address = "Lisbon, Portugal",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D15-1075",
  doi = "10.18653/v1/D15-1075",
  pages = "632--642",
}


@incollection{esnli,
  title = {e-SNLI: Natural Language Inference with Natural Language Explanations},
  author = {Camburu, Oana-Maria and Rockt"{a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages = {9539--9549},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf}
}

@inproceedings{ensemble,
  title = "Don{'}t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases",
  author = "Clark, Christopher  and
    Yatskar, Mark  and
    Zettlemoyer, Luke",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
  month = nov,
  year = "2019",
  address = "Hong Kong, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D19-1418",
  doi = "10.18653/v1/D19-1418",
  pages = "4069--4082",
  abstract = "State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.",
}

@article{shap,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{sick,
  title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
  author = "Marelli, Marco  and
    Menini, Stefano  and
    Baroni, Marco  and
    Bentivogli, Luisa  and
    Bernardi, Raffaella  and
    Zamparelli, Roberto",
  booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
  month = may,
  year = "2014",
  address = "Reykjavik, Iceland",
  publisher = "European Language Resources Association (ELRA)",
  url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
  pages = "216--223",
}

@inproceedings{dissent,
  title = "{D}is{S}ent: Learning Sentence Representations from Explicit Discourse Relations",
  author = "Nie, Allen  and
    Bennett, Erin  and
    Goodman, Noah",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2019",
  address = "Florence, Italy",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P19-1442",
  doi = "10.18653/v1/P19-1442",
  pages = "4497--4510",
  abstract = "Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including SentEval, and we achieve state-of-the-art results on Penn Discourse Treebank{'}s implicit relation prediction task.",
}

@inproceedings{lime,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{integratedgradients,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={International conference on machine learning},
  pages={3319--3328},
  year={2017},
  organization={PMLR}
}

@InProceedings{multinli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of
               the North American Chapter of the
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hyponly,
  title = "Hypothesis Only Baselines in Natural Language Inference",
  author = "Poliak, Adam  and
    Naradowsky, Jason  and
    Haldar, Aparajita  and
    Rudinger, Rachel  and
    Van Durme, Benjamin",
  booktitle = "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics",
  month = jun,
  year = "2018",
  address = "New Orleans, Louisiana",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/S18-2023",
  doi = "10.18653/v1/S18-2023",
  pages = "180--191",
  abstract = "We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on 10 distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.",
}

@article{mccBad,
  title = {On the performance of Matthews correlation coefficient (MCC) for imbalanced dataset},
  journal = {Pattern Recognition Letters},
  volume = {136},
  pages = {71-80},
  year = {2020},
  author = {Qiuming Zhu}
}

@Article{mccGood,
  author={Chicco, Davide and T{\"o}tsch, Niklas and Jurman, Giuseppe},
  title={The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation},
  journal={BioData Mining},
  year={2021},
  month={Feb},
  day={04},
  volume={14},
  number={1}
}

@article{mcc,
  author = {Powers, David},
  year = {2008},
  month = {01},
  pages = {},
  title = {Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness \& Correlation},
  volume = {2},
  journal = {Mach. Learn. Technol.}
}

@misc{macrof1,
  doi = {10.48550/ARXIV.1911.03347},
  url = {https://arxiv.org/abs/1911.03347},
  author = {Opitz, Juri and Burst, Sebastian},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Macro F1 and Macro F1},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@misc{shleifer2020pretrained,
      title={Pre-trained Summarization Distillation}, 
      author={Sam Shleifer and Alexander M. Rush},
      year={2020},
      eprint={2010.13002},
      primaryClass={cs.CL}
}

@inproceedings{cnn2,
    title = "Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond",
    author = "Nallapati, Ramesh  and
      Zhou, Bowen  and
      dos Santos, Cicero  and
      Gul{\c{c}}ehre, {\c{C}}a{\u{g}}lar  and
      Xiang, Bing",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1028",
    doi = "10.18653/v1/K16-1028",
    pages = "280--290",
}

@inproceedings{cnn1,
author = {Hermann, Karl Moritz and Ko\v{c}isk\'{y}, Tom\'{a}\v{s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
title = {Teaching Machines to Read and Comprehend},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1693-1701},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{yanaka-etal-2019-help,
    title = "{HELP}: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning",
    author = "Yanaka, Hitomi  and
      Mineshima, Koji  and
      Bekki, Daisuke  and
      Inui, Kentaro  and
      Sekine, Satoshi  and
      Abzianidze, Lasha  and
      Bos, Johan",
    booktitle = "Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-1027",
    doi = "10.18653/v1/S19-1027",
    pages = "250--255",
    abstract = "Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it.",
}

@book{nltk,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{curran-etal-2007-linguistically,
    title = "Linguistically Motivated Large-Scale {NLP} with {C}{\&}{C} and Boxer",
    author = "Curran, James  and
      Clark, Stephen  and
      Bos, Johan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-2009",
    pages = "33--36",
}

@inproceedings{miller-1994-wordnet,
    title = "{W}ord{N}et: A Lexical Database for {E}nglish",
    author = "Miller, George A.",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",
    year = "1994",
    url = "https://aclanthology.org/H94-1111",
}
