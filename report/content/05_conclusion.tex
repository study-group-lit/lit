\section{Conclusion} \label{sec:conclusion}
Following the structure of the previous sections, we conclude our findings for each of the posed hypotheses.

\paragraph{H1:} \textit{\acp{PLM} do not contain enough inherent information without fine-tuning for \ac{NLI}.}

Our results clearly show that using prompting of \acp{PLM} for \ac{NLI} is not sufficient to get good predictive performance. We have tuned the prompt on a training dataset achieving better results and showing that the prompting can be improved. Our prompting results are still far worse than simple fine-tuned models, indicating that the information contained in the \acp{PLM} is not sufficient to complete this complex task, but additional training is needed to learn the correct reasoning patterns. With further tuning of the prompts, better results might be achieved, but our current results indicate that most of the predictive performance for \ac{NLI} is obtained from fine-tuning. This indicates that the fine-tuning procedure should be changed to improve the properties of the resulting model.

\paragraph{H2:} \textit{Fine-tuned models are biased for some linguistic phenomena.}

By comparing the predictive performance across different linguistic phenomena, we show differences in accuracy for certain linguistic phenomena. This correlation between the presence of certain linguistic phenomena and accuracy indicates a bias in the model. Furthermore, we analyze the plausibility of explanations of the model's predictions, showing that the predictions are mostly plausibly explained but with a clear difference between linguistic phenomena. Plausibility is low for classes with low predictive performance, indicating a correlation between predictive performance and bias of the model in some cases. From the evidence gathered, we conclude that the model is indeed biased for numerals and quantifiers.

\paragraph{H3:} \textit{The chosen training data is biased and the found bias is not distributed uniformly over linguistic phenomena.}

By training a hypothesis-only model and showing better performance than could be explained by a majority-only baseline, we clearly show biases in the chosen training data. By analyzing it for different linguistic phenomena, we show that the examples are most biased for quantifiers and focus our efforts on mitigating bias on this phenomenon. 

\paragraph{H4:} \textit{Mitigating biased data during training results in models with greater predictive performance and less bias.}

From the presented results, this hypothesis can be confirmed. We have shown, that either filtering biased samples or mitigating biased samples by use of a special training procedure, both result in greater predictive performance. Furthermore, we have shown, that for the model trained on filtered data, the explanations generated by interpretability methods are more plausible, indicating reduced bias. Nonetheless, it must be noted, that using filtering must be tuned in both the amount of filtering and training length, complicating the training procedure.

\paragraph{H5:} \textit{Using additional data during training for a specific linguistic phenomenon the model is biased for results in models with greater predictive performance and less bias.}

This hypothesis can be confirmed only partially. We have shown that using additional training data can result in greater predictive performance for all linguistic phenomena, even though only data specifically tuned for one phenomenon is added. Nonetheless, performance on the hard evaluation set is greatly decreased by the additional training data. Seeing that predictive performance is increased for \acs{e-SNLI} and decreased for \acs{SICK}, we interpret the model as being more biased. This could be mitigated by applying additional debiasing methods in combination with the additional recast data.

\paragraph{Summary of our Results}

We pose the following learnings from our results:

\begin{enumerate}
    \item \acp{LM} for \ac{NLI} need to be fine-tuned for this task.
    \item \ac{NLI} models get their performance from the fine-tuning procedure, which can be adjusted to modify the final predictive performance and biases of the model.
    \item Higher plausibility of the explanations of the model results in higher predictive performance on out-of-domain datasets.
    \item Current \acp{LM} are biased for quantifiers and numerals, as those phenomena sometimes require complex reasoning.
    \item Training data for \ac{NLI} is biased in multiple ways.
    \item \acs{LIME} is the explainer with the biggest faithfulness for our experiments by a significant margin.
    \item Sometimes, multiple plausibility metrics are needed to get a complete picture.
    \item Mitigating bias during training by filtering biased samples improves final predictive performance and reduces bias in the explanations.
    \item The amount of filtering must be tested and tuned to achieve improvements.
    \item Using additional data can significantly improve performance.
    \item Additional data needs to be of high quality to improve performance for hard datasets such as \acs{SICK}.
\end{enumerate}