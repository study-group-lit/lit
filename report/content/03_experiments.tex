\section{Models and Data Sets} \label{sec:models_datasets}
This section describes the existing models and datasets we use, to perform our experiments and as bases from which we create our models and datasets.

\subsection{Models}
All experiments and variants of the \acs{NLI}-model are based on the pre-trained \acf{RoBERTa} \cite{roberta} in the variation \texttt{roberta-base}, as this provides a good tradeoff of high downstream performance and lower computational requirements. The default pre-trained model is used for the prompt task. All fine-tuned models are based on the pre-trained model with an additional classification head based on the pooled token representation. The classification head is a multilayer perceptron with a single hidden layer of size $768$ and the pooled representation is obtained from the first \texttt{<s>}-token in the output of the \acs{RoBERTa} model. This \texttt{<s>}-token is the equivalent of \acs{RoBERTa} to the \texttt{CLS}-token of other models.

\begin{table}[ht]
    \centering
    \caption{Class distributions for the datasets used}
    \begin{tabular}{l c c c}
        \toprule
        \multicolumn{1}{c}{Gold Label} & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \midrule
        Entailment & $137841$ & $2821$ & $190113$ \\
        Neutral & $137152$ & $5595$ & $189218$ \\
        Contradiction & $137356$ & $1424$ & $189702$ \\
        \bottomrule
    \end{tabular}
    \label{tab:datasets:classes}
\end{table}

To create summaries of the articles in the data to be recast, we use a fine-tuned and distilled version of BART \cite{lewis-etal-2020-bart} for text summarization trained on CNN and Daily Mail articles \cite{cnn1,cnn2}. \cite{shleifer2020pretrained} We use a model with all six encoder layers from BART and just six decoder layers named \texttt{distilbart-6-6-cnn}. We use the version trained on CNN/Daily Mail, as this dataset is based on the same data, that we use in our recasting. This implies that the model should work well for the data even when distilled, as there is no shift in domain shift.

\subsection{Datasets} \label{par:models_datasets:datasets}
We use \acs{MultiNLI} \cite{multinli}, \acs{e-SNLI} \cite{esnli} and \acs{SICK} \cite{sick}. In the following, the datasets are described in more detail. Statistics of the datasets can be seen in \autoref{tab:datasets:classes} and \autoref{tab:datasets:sizes}. \autoref{tab:datasets:classes} gives an overview of the distribution of the classes for the datasets and \autoref{tab:datasets:sizes} an overview of the dataset sizes with their respective dataset splits.


\begin{table}[h]
    \centering
    \caption{Dataset split sizes. \acs{MultiNLI} shows the matched/mismatched validation sizes.}
    \begin{tabular}{l  c c c}
        \toprule
        \multicolumn{1}{c}{Split} & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \midrule
        Train & $392702$ & $4439$ & $549367$ \\
        Validation & $9815$/$9832$ & $495$ & $9842$ \\
        Test & - & $4906$ & $9824$ \\
        \bottomrule
    \end{tabular}
    \label{tab:datasets:sizes}
\end{table}

\Acf{MultiNLI} \cite{multinli} is a very large corpus that improves upon the \acs{SNLI} corpus by collecting premise-hypothesis pairs from ten different domains. Additionally, only five genres are included in the training dataset and two different validation datasets are provided. One of the validation datasets consists of the same genres as the training dataset while the other validation dataset consists of pairs from five different genres. This allows for cross-domain evaluation and comparisons to in-domain evaluation. Furthermore, including training data from multiple genres is hypothesized to reduce linguistic bias.

\begin{lstlisting}[
    language=json,
    caption={Relevant features of a random data sample from \acs{MultiNLI}.},
    label=code:data:samples:multinli
    ]
{
  "hypothesis": "Product and geography are what...",
  "premise": "Conceptually cream skimming has...",
  "label": 1,
  ...
}
\end{lstlisting}

\autoref{code:data:samples:multinli} shows relevant features of a random sample from the \ac{MultiNLI} dataset. Included are the hypothesis and premise as plain text and the expected label numerically encoded. Additional features such as parses of the premise and hypothesis and the genre of the pair are included in the dataset but irrelevant to this project.

\Acf{SICK} \cite{sick} is a small corpus constructed specifically to address issues with crowd-sourced datasets. It is constructed from two source datasets that describe the same videos or images. The descriptions are first normalized and then expanded to include specific linguistic phenomena. The dataset is much smaller than \acs{SNLI} and \acs{MultiNLI} but is considered to have much higher data quality. The features present in \acs{SICK} are similar to the features present in \acs{MultiNLI}, this is the same numerical label, the premise and the hypothesis as plain text. No parses and genre indications are included, but no further interest is spent on this detail, as those features are not relevant to this project.

\Acf{e-SNLI} \cite{esnli} is a variant of the \acs{SNLI} \cite{snli} corpus that adds up to three natural language explanations and for each explanation an annotation of which words in the premise and hypothesis sentences are deemed important for correct classifications.

\begin{lstlisting}[
    language=json,
    caption={A random data sample from \acs{MultiNLI}.},
    label=code:data:samples:esnli
    ]
{
  "explanation_1": "the person is not neces...",
  "explanation_2": "",
  "explanation_3": "",
  "hypothesis": "A person is training his horse...",
  "label": 1,
  "premise": "A person on a horse jumps over a...",
  "sentence1_highlighted_1": "{}",
  "sentence1_highlighted_2": "",
  "sentence1_highlighted_3": "",
  "sentence2_highlighted_1": "3,4,5",
  "sentence2_highlighted_2": "",
  "sentence2_highlighted_3": ""
}
\end{lstlisting}

\autoref{code:data:samples:multinli} depicts a random sample from the \acs{e-SNLI} corpus including all available features. Comparing it to the features of \acs{MultiNLI} and \acs{SICK}, it is obvious that explanations and highlights of the sentences are added to the data. Up to three different explanations are included and for each explanation, the words in the premise and hypothesis that are relevant to this explanation are provided. The words are provided as indices into the premise and hypotheses starting at zero. For code simplicity, we only use the first explanation.

% TODO: sample of rc-data

Additionally, we use a question-answering dataset consisting of CNN/DailyMail articles and corresponding answers that can be entailed by the article \cite{cnn1}. The corpus formulates cloze-style questions by providing the article and a single sentence with a single word replaced by a placeholder. The original task then is to infer what entity would be correct in the place of the placeholder. To pseudonymize the dataset, named entities are replaced with abstract placeholders of the form \enquote{entity\#\#} where \enquote{\#\#} is a positive integer. This is done in the original dataset to test reading comprehension instead of world knowledge. We replace all placeholders with the original entities to provide sensible sentences and such that the model is able to use world knowledge when necessary. Nonetheless, we use the fact that the entities were recognized in the process of recasting the data, as described in \autoref{sec:meth:recasting}

\section{Experiments} \label{sec:experiments}
This section describes all experiments we conduct to evaluate our proposed hypotheses as well as the specific setup we use to perform the experiments including the metrics we use and the splits of our datasets.

\subsection{Testing H1}
\textbf{H1}: \textit{\acp{PLM} do not contain enough inherent information without fine-tuning for \ac{NLI}.}

We test this hypothesis by comparing the predictive performance of a \ac{PLM} and comparing it to the performance of a fine-tuned \ac{LM} for \ac{NLI}. We do that by first evaluating the prompting approach using the pre-trained \ac{RoBERTa} and our three different word groups that map to the different classes, as explained in \autoref{sec:meth:probing}. Additionally, we fine-tune a pre-trained \ac{RoBERTa} model specifically on \ac{NLI} data. 

By comparing the predictive performance of both approaches we can show that fine-tuning is necessary as well as that most of the predictive performance is obtained during fine-tuning. If the hypothesis is confirmed, we would conclude that the fine-tuning procedure should be adjusted instead of the pre-training procedure to obtain better results on \ac{NLI}.

\subsection{Testing H2}
\textbf{H2}: \textit{Fine-tuned models are biased for some linguistic phenomena.}

To test this hypothesis, we evaluate the predictive performance and bias of the model for different subsets of an evaluation dataset. The dataset as well as the metrics for measuring bias are described in \autoref{sec:exp:eval:bias}. Each subset contains only records that represent a particular linguistic phenomenon. This partitioning of the dataset allows for determining which linguistic phenomena the respective model copes better with or worse. If the model performance is significantly better or worse for some linguistic phenomena, we can conclude that differences and biases based on the linguistic phenomena might exist.

The dataset is split into subsets of linguistic phenomena by us using data from WordNet \cite{miller-1994-wordnet}. We consider the linguistic phenomena of synonyms, antonyms, hypernyms, hyponyms, co-hyponyms, quantifiers and numerals. We classify a sample as corresponding to a subset when the phenomenon occurs at least once in words deemed important. We detect synonyms and antonyms by considering, for each important word, all of its synonyms or antonyms respectively and counting occurrences of those in all other words. Hypernyms, hyponyms and co-hyponyms are detected in the same way, where co-hyponyms are words with a common hypernym. Quantifiers are detected in the same way as in our method for recasting data, as described in \autoref{sec:meth:recasting}. Lastly, numerals, that is, numbers and words representing numerical quantities are found by considering the part of speech tags produced by \texttt{nltk} \cite{nltk}.

\subsection{Testing H3}
\textbf{H3}: \textit{The chosen training data is biased and the found bias is not distributed uniformly over linguistic phenomena.}

To test for biases in the training data, we train multiple hypothesis-only models, as described in \autoref{sec:method:detecting_biased_data}. The training data is said to be biased if the hypothesis-only models achieve better performance than could be explained by a majority-only baseline. The bias might manifest in particular words in the hypothesis correlating with specific labels or other structures or phenomena being correlated with a label. This is a data bias, as this is never based on reasoning if the premise can be entailed by the hypothesis, but are always based on statistical correlations only present in this particular data. By comparing the predictive performance of the hypothesis-only model for different linguistic phenomena, as described in the previous section, we can further analyze, for which linguistic phenomena the data is most biased. By analyzing in the described manner, we can learn what linguistic phenomena our mitigation needs to be focussed on.

\subsection{Testing H4}
\textbf{H4}: \textit{Mitigating biased data during training results in models with greater predictive performance and less bias.}

To test \textbf{H4}, we train three different models and evaluate their predictive performance and bias for different linguistic phenomena. All of the models are based on the methods for mitigating data bias described in \autoref{sec:method:detecting_biased_data}. 

We train two models with filtered datasets. The dataset with all samples removed that are correctly classified by two out of three hypothesis-only models is called \enquote{Filtered 2/3}, the model trained on this dataset is called the same in the following. In the same manner, the dataset and with all samples removed that are correctly classified by all three of the three hypothesis-only models the model trained on it are called \enquote{Filtered 3/3}. As the filtered datasets are smaller and thus, training for the same amount of epochs results in fewer training iterations, we also test the performance of models trained for double the amount of epochs. Those models are called \enquote{Filtered 2/3 longer} and \enquote{Filtered 3/3 longer} in the following.

The third variant we try is based on an ensemble of a frozen biased hypothesis-only model and the fine-tuning model, as previously described. The resulting model is called \enquote{Ensembled}.

\subsection{Testing H5}
\textbf{H5}: \textit{Using additional data during training for a specific linguistic phenomenon the model is biased for results in models with greater predictive performance and less bias.}

To test the effect of additional training data for a specific linguistic phenomenon, we add the recast data obtained using the method described in \autoref{sec:meth:recasting} to the original training dataset. We do not use the filtered dataset from the previous section to test these hypotheses independently of each other. More specifically, we are interested in the performance improvements that can be obtained for that specific linguistic phenomenon, for which we add data and the effects on performance for other linguistic phenomena. To this end, we measure both predictive performance and bias per linguistic phenomenon.

\autoref{tab:new_datasets:classes} gives an overview of the class distributions of our modified datasets, including the filtered datasets from the previous section. \enquote{Recast} is the name of our training dataset with recast data added. Later, the model trained on that dataset will be identified using the name.

\begin{table}[ht]
    \centering
    \caption{Class distributions for the generated datasets}
    \small
    \begin{tabular}{l c c c}
        \toprule
        \multicolumn{1}{c}{Gold Label} &  Filtered 2/3 & Filtered 3/3 & Recast \\
        \midrule
        Entailment & $54147$ & $72939$ & $174531$ \\
        Neutral & $63768$ & $85664$ & $155324$ \\
        Contradiction & $73408$ & $90628$ & $173908$ \\
        \bottomrule
    \end{tabular}
    \label{tab:new_datasets:classes}
\end{table}

\subsection{Evaluation}
To measure the quality of the models, they are tested in two different aspects: The predictive performance and the biases are measured. In the following, we describe our evaluation procedure in more detail.

In general, all models are trained on the train split of \acs{MultiNLI}, unless specified otherwise. All hyperparameters are chosen using short test runs and using the predictive performance on the validation data of \acs{MultiNLI}. The models are trained for three full epochs with a learning rate of $2 \cdot 10^-5$, no weight decay and an effective batch size of $16$.

\paragraph{Predictive Performance}
We define predictive performance as a measure of how often the prediction made by the model is correct. We measure predictive performance based on a dataset of samples, each containing a hypothesis and premise with the gold label known. The gold label is the correct entailment class of the sample.

We use the validation split of \ac{SICK} as our evaluation set. We use \ac{SICK} for all main testing of predictive performance instead of \ac{e-SNLI} or \ac{MultiNLI}, as it is a harder test set and less biased than both of those. We use the validation split instead of the training split, as we wanted to leave us the option to extend our training dataset by the training split of \ac{SICK}. For tests split by linguistic phenomena, we are using the validation split of \ac{e-SNLI}. As we only want to detect a linguistic phenomenon if it was deemed important for the task, we need a dataset with annotated important words and our only dataset with such annotations is \ac{e-SNLI}.

As the metrics for evaluation, we use the F$_1$-Score and the \ac{MCC} \cite{mcc}. The F$_1$-Score is defined as the harmonic mean of the precision $P$ and recall $R$ for a particular class. So, based on the true positives $\mathrm{TP}$, true negatives $\mathrm{TN}$, false positives $\mathrm{FP}$ and false negatives $\mathrm{FN}$, the F$_1$-Score can be calculated as 

$$\text{F}_1 = 2\cdot \frac{P \cdot R}{P + R} = \frac{2 \cdot \mathrm{TP}}{2\cdot \mathrm{TP} + \mathrm{FP} + \mathrm{FN}}.$$

For multi-class problems such as ours, we compute the macro-average, as this treats each class the same way, independently of their number of samples. The macro-average is computed by first computing the F$_1$-Score for each class and then averaging those. The \ac{MCC} is a measure similar to the Pearson correlation between the prediction and the true label. For the case of two classes, it is defined as 

{\small $$\text{MCC} = \frac{\mathrm
{TP} \cdot \mathrm{TN} - \mathrm{FP} \cdot \mathrm{FN}}{\sqrt{(\mathrm{TP}+\mathrm{FP})(\mathrm{TP}+\mathrm{FN})(\mathrm{TN}+\mathrm{FP})(\mathrm{TN}+\mathrm{FN})}},$$}

and we refer the reader to \citet{mccMultiClass} for the definition of the multi-class variant, as it is not a simple micro- or macro-average but generalized differently. The value range of the \ac{MCC} is $[-1, 1]$, where $-1$ is perfect disagreement, $0$ shows no correlation between prediction and label and $1$ is perfect agreement. Only a perfect classifier can achieve a score of $1$ and any large negative score indicates implementation errors or huge transferability problems between training and test set.

We use the \ac{MCC} and F$_1$-Score, as our evaluation set is heavily imbalanced and biased towards the class \texttt{neutral}, as can be seen in \autoref{tab:datasets:classes}. For imbalanced datasets, metrics such as accuracy might provide a flawed picture of the predictive performance. The macro F$_1$-Score is a popular metric for imbalanced datasets, that we use to alleviate that. Additionally, we use the \ac{MCC}, as it was shown that it is better suited for imbalanced datasets \cite{mccGood}. We use two metrics to provide a more complete picture of the predictive performance than can be summed up into a single number.

% TODO: do we have enough time to do additional tests on the SICK test split?

\paragraph{Measuring Bias} \label{sec:exp:eval:bias}
To assess the bias of our models, we define bias as making predictions for wrong reasons. As we cannot truthfully know the reasons for a prediction, 

% TODO: formulate sentences and stitch a coherent text from them

Defining bias as the plausibility of explanation using explanation methods

Why those explanation methods? Because it is not clear, which is best and we want to use every possible angle
Plausibility+Faithfulness metrics based on validation split of e-SNLI
What metrics? All
Why? No clear best metric, get the full picture

Additionally, it is ensured that the models come to their predictions for the right reasons by applying the interpretability methods described here. These tests are conducted on subsets of the \ac{e-SNLI} dataset. 