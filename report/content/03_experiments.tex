\section{Models and Data Sets} \label{sec:models_datasets}
This section describes the existing models and datasets we use, to perform our experiments and as bases from which we create our models and datasets.

\subsection{Models}
All experiments and variants of the \acs{NLI}-model are based on the pre-trained \acf{RoBERTa} \cite{roberta} in the variation \texttt{roberta-base}, as this provides a good tradeoff of high downstream performance and lower computational requirements. The default pre-trained model is used for the prompt task. All fine-tuned models are based on the pre-trained model with an additional classification head based on the pooled token representation. The classification head is a multilayer perceptron with a single hidden layer of size $768$ and the pooled representation is obtained from the first \texttt{<s>}-token in the output of the \acs{RoBERTa} model. This \texttt{<s>}-token is the equivalent of \acs{RoBERTa} to the \texttt{CLS}-token of other models.

\begin{table}[ht]
    \centering
    \caption{Class distributions for the datasets used}
    \begin{tabular}{l c c c}
        \toprule
        \multicolumn{1}{c}{Gold Label} & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \midrule
        Entailment & $137841$ & $2821$ & $190113$ \\
        Neutral & $137152$ & $5595$ & $189218$ \\
        Contradiction & $137356$ & $1424$ & $189702$ \\
        \bottomrule
    \end{tabular}
    \label{tab:datasets:classes}
\end{table}

To create summaries of the articles in the data to be recast, we use a fine-tuned and distilled version of BART \cite{lewis-etal-2020-bart} for text summarization trained on CNN and Daily Mail articles \cite{cnn1,cnn2}. \cite{shleifer2020pretrained} We use a model with all six encoder layers from BART and just six decoder layers named \texttt{distilbart-6-6-cnn}. We use the version trained on CNN/Daily Mail, as this dataset is based on the same data, that we use in our recasting. This implies that the model should work well for the data even when distilled, as there is no shift in domain.

\subsection{Datasets} \label{par:models_datasets:datasets}
We use \acs{MultiNLI} \cite{multinli}, \acs{e-SNLI} \cite{esnli} and \acs{SICK} \cite{sick}. In the following, the datasets are described in more detail. Statistics of the datasets can be seen in \autoref{tab:datasets:classes} and \autoref{tab:datasets:sizes}. \autoref{tab:datasets:classes} gives an overview of the distribution of the classes for the datasets and \autoref{tab:datasets:sizes} an overview of the dataset sizes with their respective dataset splits.


\begin{table}[h]
    \centering
    \caption{Dataset split sizes. \acs{MultiNLI} shows the matched/mismatched validation sizes.}
    \begin{tabular}{l  c c c}
        \toprule
        \multicolumn{1}{c}{Split} & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \midrule
        Train & $392702$ & $4439$ & $549367$ \\
        Validation & $9815$/$9832$ & $495$ & $9842$ \\
        Test & - & $4906$ & $9824$ \\
        \bottomrule
    \end{tabular}
    \label{tab:datasets:sizes}
\end{table}

\Acf{MultiNLI} \cite{multinli} is a very large corpus that improves upon the \acs{SNLI} corpus by collecting premise-hypothesis pairs from ten different domains. Additionally, only five genres are included in the training dataset and two different validation datasets are provided. One of the validation datasets consists of the same genres as the training dataset, while the other validation dataset consists of pairs from five different genres. This allows for cross-domain evaluation and comparisons to in-domain evaluation. Furthermore, including training data from multiple genres is hypothesized to reduce linguistic bias \cite{multinli}.

\begin{lstlisting}[
    language=json,
    caption={Relevant features of a random data sample from \acs{MultiNLI}.},
    label=code:data:samples:multinli
    ]
{
  "hypothesis": "Product and geography are what...",
  "premise": "Conceptually cream skimming has...",
  "label": 1,
  ...
}
\end{lstlisting}

\autoref{code:data:samples:multinli} shows relevant features of a random sample from the \ac{MultiNLI} dataset. Included are the hypothesis and premise as plain text and the expected label numerically encoded. Additional features such as parses of the premise and hypothesis and the genre of the pair are included in the dataset but irrelevant to this project.

\Acf{SICK} \cite{sick} is a small corpus constructed specifically to address issues with crowd-sourced datasets. It is constructed from two source datasets that describe the same videos or images. The descriptions are first normalized and then expanded to include specific linguistic phenomena. The dataset is much smaller than \acs{SNLI} and \acs{MultiNLI} but is considered to have much higher data quality. The features present in \acs{SICK} are similar to the features present in \acs{MultiNLI}, this is the same numerical label, the premise and the hypothesis as plain text. No parses and genre indications are included, but no further interest is spent on this detail, as those features are not relevant to this project.

\Acf{e-SNLI} \cite{esnli} is a variant of the \acs{SNLI} \cite{snli} corpus that adds up to three natural language explanations and for each explanation an annotation of which words in the premise and hypothesis sentences are deemed important for correct classifications.

\begin{lstlisting}[
    language=json,
    caption={A random data sample from \acs{MultiNLI}.},
    label=code:data:samples:esnli
    ]
{
  "explanation_1": "the person is not neces...",
  "explanation_2": "",
  "explanation_3": "",
  "hypothesis": "A person is training his horse...",
  "label": 1,
  "premise": "A person on a horse jumps over a...",
  "sentence1_highlighted_1": "{}",
  "sentence1_highlighted_2": "",
  "sentence1_highlighted_3": "",
  "sentence2_highlighted_1": "3,4,5",
  "sentence2_highlighted_2": "",
  "sentence2_highlighted_3": ""
}
\end{lstlisting}

\autoref{code:data:samples:multinli} depicts a random sample from the \acs{e-SNLI} corpus including all available features. Comparing it to the features of \acs{MultiNLI} and \acs{SICK}, it is obvious that explanations and highlights of the sentences are added to the data. Up to three different explanations are included and for each explanation, the words in the premise and hypothesis that are relevant to this explanation are provided. The words are provided as indices into the premise and hypotheses starting at zero. For code simplicity, we only use the first explanation.

Additionally, we use a question-answering dataset consisting of CNN/DailyMail articles and corresponding answers that can be entailed by the article \cite{cnn1}. The corpus formulates cloze-style questions by providing the article and a single sentence with a single word replaced by a placeholder. The original task then is to infer what entity would be correct in the place of the placeholder. To pseudonymize the dataset, named entities are replaced with abstract placeholders of the form \enquote{entity\#\#} where \enquote{\#\#} is a positive integer. This is done in the original dataset to test reading comprehension instead of world knowledge.
\begin{lstlisting}[
    language=plain,
    caption={Format of the CNN/Dailymail dataset.},
    label=code:data:samples:cnn-format
]
<URL>

<Article>

<Question>

<Correct Entity>

<Entity Mapping>
\end{lstlisting}

Each sample in the dataset is provided in the format shown in \autoref{code:data:samples:cnn-format} as illustrated by the sample from the CNN dataset in \autoref{code:data:samples:cnn}.

\begin{lstlisting}[
    language=plain,
    caption={A random data sample from the CNN dataset.},
    label=code:data:samples:cnn
]
the job by both @entity25 and @entity26 majority leaders , @entity2 explained . @entity27 leader @entity27 of @entity28 praised @entity9 's smarts and his devotion to the @entity13 . the soft - spoken and bookish...

@placeholder will replace @entity9 , who was parliamentarian for 18 years

@entity8

@entity31:Frumin
@entity2:Reid
@entity1:CNN
@entity0:Washington
@entity13:Senate
@entity27:Mitch McConnell
...
\end{lstlisting}

We replace all placeholders in the article and question with the original entities to provide sensible sentences and such that the model is able to use world knowledge when necessary. Nonetheless, we use the fact that the entities were recognized in the process of recasting the data, as described in \autoref{sec:meth:recasting}

\section{Experiments} \label{sec:experiments}
This section describes all experiments we conduct to evaluate our proposed hypotheses as well as the specific setup we use to perform the experiments including the metrics we use and the splits of our datasets.

\subsection{Testing H1}
\textbf{H1}: \textit{\acp{PLM} do not contain enough inherent information without fine-tuning for \ac{NLI}.}

We test this hypothesis by comparing the predictive performance of a \ac{PLM} and comparing it to the performance of a fine-tuned \ac{LM} for \ac{NLI}. We do that by first evaluating the prompting approach using the pre-trained \ac{RoBERTa} and our three different word groups that map to the different classes, as explained in \autoref{sec:meth:probing}. Additionally, we fine-tune a pre-trained \ac{RoBERTa} model specifically on \ac{NLI} data. 

By comparing the predictive performance of both approaches we can show that fine-tuning is necessary as well as that most of the predictive performance is obtained during fine-tuning. If the hypothesis is confirmed, we would conclude that the fine-tuning procedure should be adjusted instead of the pre-training procedure to obtain better results on \ac{NLI}.

\subsection{Testing H2}
\textbf{H2}: \textit{Fine-tuned models are biased for some linguistic phenomena.}

To test this hypothesis, we evaluate the predictive performance and bias of the model for different subsets of an evaluation dataset. The dataset as well as the metrics for measuring bias are described in \autoref{sec:exp:eval:bias}. Each subset contains only records that represent a particular linguistic phenomenon. This partitioning of the dataset allows for determining which linguistic phenomena the respective model copes better with or worse. If the model performance is significantly better or worse for some linguistic phenomena, we can conclude that differences and biases based on the linguistic phenomena might exist.

The dataset is split into subsets of linguistic phenomena by us using data from WordNet \cite{miller-1994-wordnet}. We consider the linguistic phenomena of synonyms, antonyms, hypernyms, hyponyms, co-hyponyms, quantifiers and numerals. We classify a sample as corresponding to a subset when the phenomenon occurs at least once in words deemed important. We detect synonyms and antonyms by considering, for each important word, all of its synonyms or antonyms respectively and counting occurrences of those in all other words. Hypernyms, hyponyms and co-hyponyms are detected in the same way, where co-hyponyms are words with a common hypernym. Quantifiers are detected in the same way as in our method for recasting data, as described in \autoref{sec:meth:recasting}. Lastly, numerals are found by considering the part of speech tags produced by \texttt{nltk} \cite{nltk}. Numerals are both numbers and words describing numerical quantities.

\subsection{Testing H3}
\textbf{H3}: \textit{The chosen training data is biased and the found bias is not distributed uniformly over linguistic phenomena.}

To test for biases in the training data, we train multiple hypothesis-only models, as described in \autoref{sec:method:detecting_biased_data}. The training data is said to be biased if the hypothesis-only models achieve better performance than could be explained by a majority-only baseline. The bias might manifest in particular words in the hypothesis correlating with specific labels or other structures or phenomena being correlated with a label. This is a data bias, as this is never based on reasoning if the premise can be entailed by the hypothesis, but are always based on statistical correlations only present in this particular data. By comparing the predictive performance of the hypothesis-only model for different linguistic phenomena, as described in the previous section, we can further analyze, for which linguistic phenomena the data is most biased. By analyzing in the described manner, we can learn what linguistic phenomena our mitigation needs to be focussed on.

\subsection{Testing H4} \label{sub:experiments-h4}
\textbf{H4}: \textit{Mitigating biased data during training results in models with greater predictive performance and less bias.}

To test \textbf{H4}, we train three different models and evaluate their predictive performance and bias for different linguistic phenomena. All of the models are based on the methods for mitigating data bias described in \autoref{sec:method:detecting_biased_data}. 

We train two models with filtered datasets. The dataset with all samples removed that are correctly classified by two out of three hypothesis-only models is called \enquote{Filtered 2/3}, the model trained on this dataset is called the same in the following. In the same manner, the dataset and with all samples removed that are correctly classified by all three of the three hypothesis-only models the model trained on it are called \enquote{Filtered 3/3}. As the filtered datasets are smaller and thus, training for the same amount of epochs results in fewer training iterations, we also test the performance of models trained for double the amount of epochs. Those models are called \enquote{Filtered 2/3 longer} and \enquote{Filtered 3/3 longer} in the following.

The third variant we try is based on an ensemble of a frozen biased hypothesis-only model and the fine-tuning model, as previously described. The resulting model is called \enquote{Ensembled}.

\subsection{Testing H5}
\textbf{H5}: \textit{Using additional data during training for a specific linguistic phenomenon the model is biased for results in models with greater predictive performance and less bias.}

To test the effect of additional training data for a specific linguistic phenomenon, we add the recast data obtained using the method described in \autoref{sec:meth:recasting} to the original training dataset. We do not use the filtered dataset from the previous section to test these hypotheses independently of each other. More specifically, we are interested in the performance improvements that can be obtained for that specific linguistic phenomenon, for which we add data and the effects on performance for other linguistic phenomena. To this end, we measure both predictive performance and bias per linguistic phenomenon.

\autoref{tab:new_datasets:classes} gives an overview of the class distributions of our modified datasets, including the filtered datasets from the previous section. \enquote{Recast} is the name of our training dataset with recast data added. Later, the model trained on that dataset will be identified using the name.

\begin{table}[ht]
    \centering
    \caption{Class distributions for the generated datasets}
    \small
    \begin{tabular}{l c c c}
        \toprule
        \multicolumn{1}{c}{Gold Label} &  Filtered 2/3 & Filtered 3/3 & Recast \\
        \midrule
        Entailment & $54147$ & $72939$ & $174531$ \\
        Neutral & $63768$ & $85664$ & $155324$ \\
        Contradiction & $73408$ & $90628$ & $173908$ \\
        \bottomrule
    \end{tabular}
    \label{tab:new_datasets:classes}
\end{table}

\subsection{Evaluation}
To measure the quality of the models, they are tested in two different aspects: The predictive performance and the biases are measured. In the following, we describe our evaluation procedure in more detail.

In general, all models are trained on the train split of \acs{MultiNLI}, unless specified otherwise. All hyperparameters are chosen using short test runs and using the predictive performance on the validation data of \acs{MultiNLI}. The models are trained for three full epochs with a learning rate of $2 \cdot 10^-5$, no weight decay and an effective batch size of $16$.

\paragraph{Predictive Performance}
We define predictive performance as a measure of how often the prediction made by the model is correct. We measure predictive performance based on a dataset of samples, each containing a hypothesis and premise with the gold label known. The gold label is the correct entailment class of the sample.

We use the validation split of \ac{SICK} as our evaluation set during development and the test split of \ac{SICK} for all final results reported here. We use \ac{SICK} for all main testing of predictive performance instead of \ac{e-SNLI} or \ac{MultiNLI}, as it is a harder test set and less biased than both of those. We use the test and validation splits instead of the training split, as we wanted to leave us the option to extend our training dataset by the training split of \ac{SICK}. For tests split by linguistic phenomena, we are using the validation split of \ac{e-SNLI}. As we only want to detect a linguistic phenomenon if it was deemed important for the task, we need a dataset with annotated important words and our only dataset with such annotations is \ac{e-SNLI}.

As the metrics for evaluation, we use the F$_1$-Score and the \ac{MCC} \cite{mcc}. The F$_1$-Score is defined as the harmonic mean of the precision $P$ and recall $R$ for a particular class. So, based on the true positives $\mathrm{TP}$, true negatives $\mathrm{TN}$, false positives $\mathrm{FP}$ and false negatives $\mathrm{FN}$, the F$_1$-Score can be calculated as 

$$\text{F}_1 = 2\cdot \frac{P \cdot R}{P + R} = \frac{2 \cdot \mathrm{TP}}{2\cdot \mathrm{TP} + \mathrm{FP} + \mathrm{FN}}.$$

For multi-class problems such as ours, we compute the macro-average, as this treats each class the same way, independently of their number of samples. The macro-average is computed by first computing the F$_1$-Score for each class and then averaging those. The \ac{MCC} is a measure similar to the Pearson correlation between the prediction and the true label. For the case of two classes, it is defined as 

{\small $$\text{MCC} = \frac{\mathrm
{TP} \cdot \mathrm{TN} - \mathrm{FP} \cdot \mathrm{FN}}{\sqrt{(\mathrm{TP}+\mathrm{FP})(\mathrm{TP}+\mathrm{FN})(\mathrm{TN}+\mathrm{FP})(\mathrm{TN}+\mathrm{FN})}},$$}

and we refer the reader to \citet{mccMultiClass} for the definition of the multi-class variant, as it is not a simple micro- or macro-average but generalized differently. The value range of the \ac{MCC} is $[-1, 1]$, where $-1$ is perfect disagreement, $0$ shows no correlation between prediction and label and $1$ is perfect agreement. Only a perfect classifier can achieve a score of $1$ and any large negative score indicates implementation errors or huge transferability problems between training and test set.

We use the \ac{MCC} and F$_1$-Score, as our evaluation set is heavily imbalanced and biased towards the class \texttt{neutral}, as can be seen in \autoref{tab:datasets:classes}. For imbalanced datasets, metrics such as accuracy might provide a flawed picture of the predictive performance. The macro F$_1$-Score is a popular metric for imbalanced datasets, that we use to alleviate that. Additionally, we use the \ac{MCC}, as it was shown that it is better suited for imbalanced datasets \cite{mccGood}. We use two metrics to provide a more complete picture of the predictive performance than can be summed up into a single number.

\paragraph{Measuring Bias} \label{sec:exp:eval:bias}
To assess the bias of our models, we define bias as making predictions for wrong reasons. As we cannot truthfully know the reasons for a prediction, we use interpretability methods to infer what words might have been important for the prediction. Using the interpretability methods we define bias in the model as assigning importance to words that are not important or not assigning importance to important words. We measure the importance assigned to the words by the model through interpretability methods and control for senseless explanations by also measuring the faithfulness of the explanation.

\ac{e-SNLI} provides natural language explanations for all samples and important words of the hypothesis and premise as annotated by a human annotator. \ac{e-SNLI} provides up to three different explanations for each sample but to simplify our analysis, we select only the first and discard all others. Our analysis is based on the validation split of \ac{e-SNLI}.

We begin the analysis by feeding all samples into our model and calculating the explanations of all explainers provided by ferret \cite{ferret} for each sample individually. Those are Gradients \cite{gradients}, Integrated Gradients \cite{integratedgradients}, \ac{LIME} \cite{lime} and Partition SHAP Values \cite{shap}. We do not limit ourselves to a subset of these explainers as it is not clear which explainer is best suited for all samples and we want to provide a holistic analysis covering every possible angle. For the tests over the complete datasets, we do not use Gradients as an interpretability method, as it was too costly to compute for the entire dataset.

Afterward, we evaluate the explanations. We differentiate between two aspects of the evaluation of explanations and provide different metrics to measure each of them. The first aspect is plausibility which assesses how well the explanation aligns with human reasoning. We use the plausibility to measure the bias of the model which is present by the definition above if the reasoning of the model differs from human reasoning. The metrics provided by ferret for this aspect are token-level \ac{IOU} and F$_1$-Score as well as \ac{AUPRC} calculated between the computed explanation and the human explanation provided by \ac{e-SNLI} \cite{ferret}. 

The second aspect is faithfulness which is determined by how closely the explanation represents the internal functioning of the model. We use the faithfulness metrics to determine if the calculated explanations are sensible at all. For this aspect, ferret provides the metrics of Comprehensiveness, Sufficiency and Correlation with Leave-One-Out scores.

Here, too, we do not restrict our analysis to a subset of metrics for the same reasons as above: It is not clear which metric is best suited and we want to get the full picture of metrics as every metric covers a different aspect of the quality of an explanation.

As the last step, we calculate the mean for every explainer and metric over the whole evaluation dataset. These results allow us to make an informed assessment of which explainer yields the best results and how close the reasoning of the model is to human reasoning.