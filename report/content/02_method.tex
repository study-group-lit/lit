\section{Method} \label{sec:method}
\paragraph{Probing for \acs{NLI}}
To test the inherent classification performance of \acp{PLM} on \acs{NLI}, a zero-shot baseline is tested. As the \ac{PLM} we plan to use (see \autoref{sec:models_datasets} for further information) is a \ac{MLM}, a mask prediction task is used for zero-shot testing. The template used for that task is \texttt{<premise> <mask> <hypothesis>}, where \texttt{<premise>} is the entire premise sentence with the full stop removed, \texttt{<hypothesis>} is the hypothesis sentence with the first word not capitalized and \texttt{<mask>} is the token that will be predicted.

\begin{table}[ht]
    \centering
    \caption{The discourse markers chosen for the zero-shot task with their associated predicted labels.}
    \begin{tabular}{c | c | c}
        Entailment & Neutral & Contradiction \\
        \hline
        as & and  & but \\
        because & also & although \\
        so & or & still
    \end{tabular}
    \label{tab:discourse:markers}
\end{table}

This task is inspired by the discourse prediction task introduced by \cite{dissent}, where a word to relate two sentences to each other is to be predicted. In the same way, we constrain the number of words that are relevant for this task to a short list of typical discourse markers shown in \autoref{tab:discourse:markers} and only compare predictions between those. The predicted class is then obtained by summing the probability of all discourse markers associated with a class and choosing the class with the maximum probability.

\paragraph{Fine-tuning for \acs{NLI}}

Fine-tuning for \acs{NLI} is performed by supervised training on a corpus containing premises, hypotheses and the expected labels. To predict on a single sample, both the premise and hypothesis are fed into the network separated by a separator token. The prediction is then computed by a classification head based on the pooled representation of the complete input. Classification is performed by predicting a vector with three dimensions where each dimension corresponds to one of the labels. The predicted label is the index of the maximum value in that vector. Thus the model is fine-tuned by training it to predict the correct label on the training dataset.

\paragraph{Detecting biased data}

By changing the fine-tuning process to only using the hypothesis as input of the model, biases in the data can be found. Such a hypothesis-only model can only correctly predict the labels either by chance or by abusing biases in the data -- it is never correct for the right reasons. It has been shown that for datasets currently used for fine-tuning for \acs{NLI}, hypothesis-only models can be trained that are better than a majority baseline \citep{hyponly}. Thus, it can be concluded that biases in the data must exist that facilitate correct predictions based only on the hypothesis.

We use this fact to find biased samples in the training datasets. We have fine-tuned three hypothesis-only models on the dataset and then used those to find all samples biased that at least two hypothesis-only models predict correctly.

\paragraph{Mitigating data bias}

We employ two methods to remove data bias from the training procedure. The naive method is, to simply remove all samples deemed biased from the training set. By completely removing them from the training procedure, the model cannot be biased by those samples.

An additional method is introduced by \citet{ensemble}. This method is based on using an ensemble of a frozen biased model and a main model during training and only using the fine-tuned main model during testing. By using the frozen biased model in an ensemble with the main model, the main model can learn to predict based on patterns other than those based on biases. The ensembling is done by multiplying the prediction of the biased model with the prediction of the main model. The influence of the prediction of the biased model can be reduced by a learned value that is predicted by a secondary head of the main model. By learning to always completely discount the biased model, the model might then learn the biases itself. To prevent this, an additional entropy term is added to the loss function, which punishes the model for discounting the biased prediction too much. \footnote{For more information and justification of this procedure compare with section 3.2 of \cite{ensemble}.}

\paragraph{Getting data specific to quantifiers}

As we will show in TODO, the model is especially biased for samples on quantifiers. To fight this bias, we create additional training data by recasting question-answering-data to the task of \ac{NLI}. The dataset consists of articles and single sentences that can be entailed when the placeholder is replaced by a particular entity but cannot be entailed when it is replaced with a different entity. We use those to create premise-hypothesis-pairs that are examples of entailment containing quantifiers. To ensure that the new examples help with the understanding of quantifiers, we only select those containing quantifiers. The proportion of examples containing quantifiers is shown in TODO. To create neutral and contradictory examples, we use the monotonicity of quantifiers to change either to change TODO.