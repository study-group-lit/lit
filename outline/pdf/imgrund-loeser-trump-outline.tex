\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nolist]{acronym}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{placeins}
\usepackage[nolist]{acronym}
\usepackage[style=authoryear]{biblatex}

\onehalfspacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

\addbibresource{literature.bib}

\title{Project LIT - Outline}
\author{Niklas Loeser, Erik Imgrund, Andre Trump}
\date{\today}

\begin{document}

\begin{acronym}
    \acro{LM}{Language Model}
    \acro{MCC}{Matthews correlation coefficient}
    \acro{NLI}{Natural Language Inference}
    \acro{PLM}{Pretrained Language Model}
\end{acronym}

\maketitle

\section{Task Description}
% What is NLI?
Given a premise and a hypothesis, \ac{NLI} is the task of determining, whether the hypothesis is true, false, or cannot be determined. The model respectively chooses one of the three classes \texttt{entailment}, \texttt{contradiction} or \texttt{neutral}.

% What is our aim?
Our aim is to identify linguistic biases in the ROBERTA model, fine-tuned on MultiNLI. After identifying linguistic biases we then aim to improve the model, by removing the identified biases.

% What are our hypotheses?
We pose the following hypotheses:
\begin{enumerate}
  \item The fine-tuned ROBERTA model has linguistic biases for the \ac{NLI} task.
  \item Removing linguistic biases from the datasets used for fine-tuning ROBERTA results in a less biased model.
  \item A less biased model performs worse. % Will not name accuracy here, as accuracy is bad. We can name F1 and other score later.
\end{enumerate}

\section{Method (Erik)}
How to probe for NLI?

How to fine-tune for NLI?

How do we detect biased data?

How to mitigate biases?

\section{Models and Data Sets (Erik)}
Which models do we use?

Roberta, aber welche?

Which data sets to use?

- MultiNLI
- SICK
- eSNLI

\section{Experiments (Andre)}
Baseline:
1. Zero-shot Probing auf pretrained ROBERTA
2. Für NLI fine-tuned ROBERTA mit Classification Head
Unsere:
3. ROBERTA fine-tuned nur mit Hypotheses
4. Ensemble: hypothesis-only Model freezed + standard ROBERTA fine-tuning mit hypothesis-only model
5. ROBERTA fine-tuning auf MultiNLI mit reduziertem Bias

Training:
- MultiNLI
- optional: SICK (weil wenige Daten)
Test:
- eSNLI für Interpretability, Kategorisiert nach linguistischen Phänomenen
- SICK für Accuracy, weil eSNLI hat Bias ohne Ende

\section{Analysis}
% 1. Accuracy (F1 + MCC) auf SICK und eSNLI einzeln nach Kategorien
% 2. Auf Bias überprüfen: Vergleich vom Modell für wichtig erachtete Token mit von Menschen als wichitg erachtete Tokens
% Visualisierungen:
% - Confusion Matrix (Gentrennt nach Phänomenen)
% - Tabellen Interpretability Metriken (siehe ferret)

% Accuracy
For model analysis, we choose the SICK dataset, as it is less biased than other datasets, such as MultiNLI. As we only perform analysis of the models, the smallness of SICK does not impact the results.

In addition, we choose the eSNLI dataset, as it provides explanations alongside the data, to which we can compare our results. The eSNLI data will be grouped according to linguistic phaenomena, such that we perform the analysis separately for each phenomenon.

To measure the accuracy, we provide the Macro $F_1$-score \parencite{macrof1} and \ac{MCC}\parencite{mcc}, as \ac{MCC} has proven to be more reliable than accuracy and $F_1$ \parencite{mccGood}.


% Bias
To analyze linguistic biases, we compare the eSNLI explanations to important tokens for each linguistic phenomenon. We detect important tokens using the confusion matrix for each linguistic phaenomena and interpretability metrics.

% Ferret
We use the interpretability metrics plausibility and faithfulness \parencite{attanasio2022ferret} using the integrated Gradients \parencite{sundararajan2017integratedgradients}, LIME \parencite{ribeiro2016lime} and Partition SHAP Values \parencite{lundberg2017shap} methods.

\printbibliography

\end{document}
