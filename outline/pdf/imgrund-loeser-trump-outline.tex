\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nolist]{acronym}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{placeins}
\usepackage[nolist]{acronym}
\usepackage[style=authoryear]{biblatex}

\onehalfspacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

\addbibresource{literature.bib}

\title{Project LIT - Outline}
\author{Niklas Loeser, Erik Imgrund, Andre Trump}
\date{\today}

\begin{document}

\begin{acronym}
    \acro{LM}{Language Model}
    \acro{NLI}{Natural Language Inference}
    \acro{PLM}{Pretrained Language Model}
\end{acronym}

\maketitle

\section{Task Description (Niklas)}
What is NLI?

What is our aim?

What are our hypotheses?

\section{Method (Erik)}
How to probe for NLI?

How to fine-tune for NLI?

How do we detect biased data?

How to mitigate biases?

\section{Models and Data Sets (Erik)}
Which models do we use?

Roberta, aber welche?

Which data sets to use?

- MultiNLI
- SICK
- eSNLI

\section{Experiments}
\subsection{Baseline}
We use two models as a baseline to compare our results with: A pretrained RoBERTa model will serve as zero-short baseline. Furthermore, we use RoBERTa model fine-tuned on the MultiNLI datset as fine-tuned baseline.

\subsection{Obtaining models with lower bias}
To pursue our two approaches to obtain a fine-tuned model with lower bias we then conduct the following experiments: For the first approach we fine-tune a pretrained RoBERTa model on the MultiNLI dataset after it has been preprocessed as described above in order to reduce the bias in the dataset. In case too many records of the dataset need to be filtered out because they carry a bias we use the less biased SICK dataset to increase out amount of training data. 

For the second approach we need to conduct two experiments: First, we fine-tune a pretrained RoBERTAa model only on the hypotheses of the entire MultiNLI dataset to create a biased model. The ensembled model consists of the biased model we obtained before and a standard RoBERTa model. The ensembled model is trained on the entire MultiNLI dataset.

\subsection{Test}
To measure the performance of our models we will test them in two different aspects: First, we will measure the accuracy our models archive by testing them on SICK dataset as it is less biased than the e-SNLI dataset.

Additionally, we will ensure that our models come to their predictions for the right reasons by applying the interpretability methods described in the next sections. These tests will be conducted on subsets of the e-SNLI dataset. Each subset will contain only records that represent a particular linguistic phenomenon. This partitioning of the dataset allows us to determine which linguistic phenomena the respective model copes better or worse with.

\section{Analysis (Niklas)}
1. Accuracy (F1 + MCC) auf SICK und eSNLI einzeln nach Kategorien
2. Auf Bias 체berpr체fen: Vergleich vom Modell f체r wichtig erachtete Token mit von Menschen als wichitg erachtete Tokens
Visualisierungen:
- Confusion Matrix (Gentrennt nach Ph채nomenen)
- Tabellen Interpretability Metriken (siehe ferret)


\printbibliography

\end{document}