\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nolist]{acronym}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{placeins}
\usepackage[nolist]{acronym}
\usepackage[style=authoryear]{biblatex}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{etoolbox}

\widowpenalty=10000
\clubpenalty=10000
\parfillskip 0pt plus 0.75\textwidth
\makeatletter
\patchcmd{\@sect}{\begingroup}{\begingroup\parfillskip=0pt plus 1fil\relax}{}{}
\patchcmd{\@ssect}{\begingroup}{\begingroup\parfillskip=0pt plus 1fil\relax}{}{}
\makeatother

\onehalfspacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

\addbibresource{literature.bib}

\title{Project LIT - Outline}
\author{Niklas Loeser, Erik Imgrund, Andre Trump}
\date{\today}

\input{json.tex}

\begin{document}

\begin{acronym}
    \acro{e-SNLI}{Natural Language Inference with Natural Language Explanations}
    \acro{LIME}{Local Interpretable Model-Agnostic Explanations}
    \acro{LM}{Language Model}
    \acro{MCC}{Matthews correlation coefficient}
    \acro{MLM}{Masked Language Model}
    \acro{MultiNLI}{Multi-Genre Natural Language Inference}
    \acro{NLI}{Natural Language Inference}
    \acro{PLM}{Pretrained Language Model}
    \acro{RoBERTa}{Robustly Optimized {BERT} Pretraining Approach}
    \acro{SICK}{Sentences Involving Compositional Knowldedge}
    \acro{SNLI}{Stanford Natural Language Inference}
\end{acronym}

\maketitle

\section{Task Description}
% What is NLI?
Given a premise and a hypothesis, \ac{NLI} is the task of determining, whether the hypothesis is true, false, or cannot be determined, given only the premise. If the hypothesis is true, it can be said to be \textbf{entailed} by the premise. If the hypothesis is false, it is \textbf{contradictory} to the premise. If the truthiness of the hypothesis cannot be determined by the premise, it is said to be \textbf{neutral}. Respectively, one of the three classes \texttt{entailment}, \texttt{contradiction} or \texttt{neutral} is chosen.

% What is our aim?
Our aim is to identify biases in \acp{LM} for \ac{NLI} and mitigate those biases by removing biased data in the training procedure.

% What are our hypotheses?
We pose the following hypotheses:\vspace{-1.5em}
\begin{description}
  \item[H1] The fine-tuning procedure of \acp{LM} introduces biases for the \ac{NLI} task.\vspace{-0.7em}
  \item[H2] Mitigating biases from the datasets results in less biased \acp{LM}.\vspace{-0.7em}
  \item[H3] \acp{LM} with less bias make worse predictions for biased data. \vspace{-0.7em}% Will not name accuracy here, as accuracy is bad. We can name F1 and other score later.
  \item[H4] \acp{LM} with less bias make better predictions for data without the same bias.
\end{description}

\section{Method} \label{sec:method}
\paragraph{Probing for \acs{NLI}}
To provide a zero-shot baseline for our experiments, the inherent classification performance of \acp{PLM} on \acs{NLI} is tested. As the \ac{PLM} we plan to use (see \autoref{sec:models_datasets} for further information) is a \ac{MLM}, a mask prediction task is used for zero-shot testing. The template used for that task is \texttt{<premise> <mask> <hypothesis>}, where \texttt{<premise>} is the entire premise sentence with the full stop removed, \texttt{<hypothesis>} is the hypothesis sentence and \texttt{<mask>} is the token that will be predicted.

\begin{table}[h]
    \centering
    \caption{The discourse markers chosen for the zero-shot task with their associated predicted labels.}
    \begin{tabular}{c | c | c}
        Entailment & Neutral & Contradiction \\
        \hline
        as & and  & but \\
        because & also & although \\
        so & or & still
    \end{tabular}
    \label{tab:discourse:markers}
\end{table}

This task is inspired by the discourse prediction task introduced by \cite{dissent} where a word to relate two sentences to each other is to be predicted. In the same way, we constrain the number of words that are relevant for this task to a short list of typical discourse markers shown in \autoref{tab:discourse:markers} and only compare predictions between those. The predicted class is then obtained by summing the probability of all discourse markers associated with a class and choosing the class with the maximum probability.

\paragraph{Fine-tuning for \acs{NLI}}

Fine-tuning for \acs{NLI} is performed by supervised training on a corpus containing premises, hypotheses and the expected labels. To predict on a single sample, both the premise and hypothesis are fed into the network separated by a separator token. The prediction is then computed by a classification head based on the pooled representation of the complete input. Classification is performed by predicting a vector with three dimensions where each dimension corresponds to one of the labels. The predicted label is the index of the maximum value in that vector. Thus the model is fine-tuned by training it to predict the correct label on the training dataset.

\paragraph{Detecting biased data}

By changing the fine-tuning process to only using the hypothesis as input of the model, biases in the data can be found. Such a hypothesis-only model can only correctly predict the labels either by chance or by abusing biases in the data -- it is never correct for the right reasons. It has been shown that for datasets currently used for fine-tuning for \acs{NLI}, hypothesis-only models can be trained that are better than a majority baseline \parencite{hyponly}. Thus, it can be concluded that biases in the data must exist that facilitate correct predictions based only on the hypothesis.

We plan on using this fact to find biased samples in the training datasets. This can be done by first fine-tuning a hypothesis-only model on the dataset and then declaring all samples biased that a hypothesis-only model is correct on with high confidence.

\paragraph{Mitigating data bias}

We employ two methods to remove data bias from the training procedure. The naive method is, to simply remove all samples deemed biased from the training set. By completely removing them from the training procedure, the model cannot be biased by those samples.

An additional method is introduced by \cite{ensemble}. This method is based on using an ensemble of a frozen biased model and a main model during training and only using the fine-tuned main model during testing. By using the frozen biased model in an ensemble with the main model, the main model can learn to predict based on patterns other than those based on biases. The ensembling is done by multiplying the prediction of the biased model with the prediction of the main model. The influence of the prediction of the biased model can be reduced by a learned value that is predicted by a secondary head of the main model. By learning to always completely discount the biased model, the model might then learn the biases itself. To prevent this, an additional entropy term is added to the loss function, which punishes the model for discounting the biased prediction too much. \footnote{For more information and justification of this procedure compare with section 3.2 of \cite{ensemble}.}

\section{Models and Data Sets} \label{sec:models_datasets}
\paragraph{Models}
All experiments and variants are based on the pre-trained \acf{RoBERTa} \parencite{roberta} in the variation \texttt{roberta-base}, as this provides a good tradeoff of high downstream performance and lower computational requirements. The default pre-trained model is used for the prompt task. All fine-tuned models are based on the pre-trained model with an additional classification head based on the pooled token representation. The classification head is a multilayer perceptron with a single hidden layer of size $768$ and the pooled representation is obtained from the first \texttt{<s>}-token in the output of the \acs{RoBERTa} model. This \texttt{<s>}-token is the equivalent of \acs{RoBERTa} to the \texttt{CLS}-token of other models.

\begin{table}[h]
    \centering
    \caption{Class distributions for the datasets used}
    \begin{tabular}{r || c | c | c}
        & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \hline
        Entailment & $137841$ & $2821$ & $190113$ \\
        Neutral & $137152$ & $5595$ & $189218$ \\
        Contradiction & $137356$ & $1424$ & $189702$
    \end{tabular}
    \label{tab:datasets:classes}
\end{table}

\paragraph{Datasets} We use \acs{MultiNLI} \parencite{multinli}, \acs{e-SNLI} \parencite{esnli} and \acs{SICK} \parencite{sick}. In the following, the datasets are described in more detail. Statistics of the datasets can be seen in \autoref{tab:datasets:classes} and \autoref{tab:datasets:sizes}. \autoref{tab:datasets:classes} gives an overview of the distribution of the classes for the datasets and \autoref{tab:datasets:sizes} an overview of the dataset sizes with their respective dataset splits.

\begin{table}[h]
    \centering
    \caption{Dataset split sizes. \acs{MultiNLI} shows the matched/mismatched validation sizes.}
    \begin{tabular}{r || c | c | c}
        & \acs{MultiNLI} & \acs{SICK} & \acs{e-SNLI} \\
        \hline
        Train & $392702$ & $4439$ & $549367$ \\
        Validation & $9815$/$9832$ & $495$ & $9842$ \\
        Test & - & $4906$ & $9824$
    \end{tabular}
    \label{tab:datasets:sizes}
\end{table}

\Acf{MultiNLI} \parencite{multinli} is a very large corpus that improves upon the \acs{SNLI} corpus by collecting premise-hypothesis pairs from ten different domains. Additionally, only five genres are included in the training dataset and two different validation datasets are provided. One of the validation datasets consists of the same genres as the training dataset while the other validation dataset consists of pairs from five different genres. This allows for cross-domain evaluation and comparisons to in-domain evaluation. Furthermore, including training data from multiple genres is hypothesized to reduce linguistic bias.

\begin{lstlisting}[
    language=json,
    caption={Relevant features of a random data sample from \acs{MultiNLI}.},
    label=code:data:samples:multinli
    ]
{
  "hypothesis": "Product and geography are what...",
  "premise": "Conceptually cream skimming has...",
  "label": 1,
  ...
}
\end{lstlisting}

\autoref{code:data:samples:multinli} shows relevant features of a random sample from the \ac{MultiNLI} dataset. Included are the hypothesis and premise as plain text and the expected label numerically encoded. Additional features such as parses of the premise and hypothesis and the genre of the pair are included in the dataset but irrelevant to this project.

\Acf{SICK} \parencite{sick} is a small corpus constructed specifically to address issues with crowd-sourced datasets. It is constructed from two source datasets that describe the same videos or images. The descriptions are first normalized and then expanded to include specific linguistic phenomena. The dataset is much smaller than \acs{SNLI} and \acs{MultiNLI} but is considered to have much higher data quality. The features present in \acs{SICK} are similar to the features present in \acs{MultiNLI}, this is the same numerical label, the premise and the hypothesis as plain text. No parses and genre indications are included, but no further interest is spent on this detail, as those features are not relevant to this project.

\Acf{e-SNLI} \parencite{esnli} is a variant of the \acs{SNLI} \parencite{snli} corpus that adds up to three natural language explanations and for each explanation an annotation of which words in the premise and hypothesis sentences are deemed important for correct classifications.

\begin{lstlisting}[
    language=json,
    caption={A random data sample from \acs{MultiNLI}.},
    label=code:data:samples:esnli
    ]
{
  "explanation_1": "the person is not neces...",
  "explanation_2": "",
  "explanation_3": "",
  "hypothesis": "A person is training his horse...",
  "label": 1,
  "premise": "A person on a horse jumps over a...",
  "sentence1_highlighted_1": "{}",
  "sentence1_highlighted_2": "",
  "sentence1_highlighted_3": "",
  "sentence2_highlighted_1": "3,4,5",
  "sentence2_highlighted_2": "",
  "sentence2_highlighted_3": ""
}
\end{lstlisting}

\autoref{code:data:samples:multinli} depicts a random sample from the \acs{e-SNLI} corpus including all available features. Comparing it to the features of \acs{MultiNLI} and \acs{SICK}, it is obvious that explanations and highlights of the sentences are added to the data. Up to three different explanations are included and for each explanation, the words in the premise and hypothesis that are relevant to this explanation are provided. The words are provided as indices into the premise and hypotheses starting at zero.

\section{Experiments}
\paragraph{Baseline} We use two models as baselines to compare against our results. A pre-trained \ac{RoBERTa} model will serve as a zero-shot baseline. Furthermore, we use a \ac{RoBERTa} model fine-tuned on the \ac{MultiNLI} dataset as a fine-tuned baseline. The comparison between our fine-tuned results and the zero-shot baseline is used to test \textbf{H1}.

\paragraph{Obtaining models with lower bias} To pursue our two approaches to obtain a fine-tuned model with less bias we conduct the following experiments: For the first approach, we fine-tune a pre-trained \ac{RoBERTa} model on the \ac{MultiNLI} dataset after it has been preprocessed as described in \autoref{sec:method} to reduce the bias in the dataset. Optionally, we can add the training data from \ac{SICK} to increase our total amount of training data. 

For the second approach we need to conduct two experiments: First, we fine-tune a pre-trained \ac{RoBERTa} model only on the hypotheses of the entire \ac{MultiNLI} dataset to create a biased model. Then an ensemble consisting of the biased model we previously obtained and a standard \ac{RoBERTa} model is trained on the entire \ac{MultiNLI} dataset as described in \autoref{sec:method}.

\paragraph{Test} To measure the quality of the models, they are tested in two different aspects: First, the predictive performance that the models achieve is measured by testing them on the SICK dataset as it is less biased than the e-SNLI dataset. Additionally, it is ensured that the models come to their predictions for the right reasons by applying the interpretability methods described in \autoref{sec:analysis}. These tests are conducted on subsets of the e-SNLI dataset. Each subset contains only records that represent a particular linguistic phenomenon. This partitioning of the dataset allows for determining which linguistic phenomena the respective model copes better with or worse.

\section{Analysis} \label{sec:analysis}
% 1. Accuracy (F1 + MCC) auf SICK und eSNLI einzeln nach Kategorien
% 2. Auf Bias überprüfen: Vergleich vom Modell für wichtig erachtete Token mit von Menschen als wichitg erachtete Tokens
% Visualisierungen:
% - Confusion Matrix (Gentrennt nach Phänomenen)
% - Tabellen Interpretability Metriken (siehe ferret)


To analyze the performance of the \acp{LM}, we test their predictive performance and their bias. We provide different scores and visualizations to test our hypothesis and provide insights into the datasets and \acp{LM}.
% Accuracy

To measure the predictive performance, we provide the macro $\text{F}_1$-score \parencite{macrof1} and \ac{MCC} \parencite{mcc}, as \ac{MCC} has proven to be more reliable than accuracy and $\text{F}_1$ \parencite{mccGood}.

% Bias
To analyze biases, we compare the words deemed important by the \ac{e-SNLI} explanations to the tokens deemed important for the model prediction by interpretability methods. Furthermore, to detect linguistic biases, we conduct a confusion analysis for each specific linguistic phenomenon.

% Ferret
To generate explanations we use the interpretability methods Integrated Gradients \parencite{integratedgradients}, \ac{LIME} \parencite{lime} and Partition SHAP Values \parencite{shap}. We use the plausibility and faithfulness \parencite{ferret} metrics to judge the model bias.

To demonstrate the biases, we provide example sentences with important tokens highlighted. Furthermore, to provide a different view on the importance of certain tokens, we visualize the attention maps.

\printbibliography{}

\end{document}
