\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nolist]{acronym}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{placeins}
\usepackage[nolist]{acronym}
\usepackage[style=authoryear]{biblatex}

\onehalfspacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

\addbibresource{literature.bib}

\title{Project LIT - Outline}
\author{Niklas Loeser, Erik Imgrund, Andre Trump}
\date{\today}

\begin{document}

\begin{acronym}
    \acro{LM}{Language Model}
    \acro{NLI}{Natural Language Inference}
    \acro{PLM}{Pretrained Language Model}
\end{acronym}

\maketitle

\section{Task Description (Niklas)}
What is NLI?

What is our aim?

What are our hypotheses?

\section{Method (Erik)}
\label{sec:method}

How to probe for NLI?

How to fine-tune for NLI?

How do we detect biased data?

How to mitigate biases?

\section{Models and Data Sets (Erik)}
Which models do we use?

Roberta, aber welche?

Which data sets to use?

- MultiNLI
- SICK
- eSNLI

\section{Experiments}
\subsection{Baseline}
We use two models as a baseline to compare our results with: A pretrained RoBERTa model will serve as a zero-shot baseline. Furthermore, we use a RoBERTa model fine-tuned on the MultiNLI dataset as a fine-tuned baseline.

\subsection{Obtaining models with lower bias}
To pursue our two approaches to obtain a fine-tuned model with a lower bias we then conduct the following experiments: For the first approach, we fine-tune a pretrained RoBERTa model on the MultiNLI dataset after it has been preprocessed as described in \autoref{sec:method} to reduce the bias in the dataset. In case too many records of the dataset need to be filtered out because they carry a bias we use the less biased SICK dataset to increase our amount of training data. 

For the second approach we need to conduct two experiments: First, we fine-tune a pretrained RoBERTAa model only on the hypotheses of the entire MultiNLI dataset to create a biased model. The ensembled model consists of the biased model we obtained before and a standard RoBERTa model. The ensembled model is trained on the entire MultiNLI dataset.

\subsection{Test}
To measure the performance of the models they are tested in two different aspects: First, the accuracy that the models archive is measured by testing them on the SICK dataset as it is less biased than the e-SNLI dataset.

Additionally, it is ensured that the models come to their predictions for the right reasons by applying the interpretability methods described in \autoref{sec:analysis}. These tests are conducted on subsets of the e-SNLI dataset. Each subset contains only records that represent a particular linguistic phenomenon. This partitioning of the dataset allows to determine which linguistic phenomena the respective model copes better or worse with.

\section{Analysis (Niklas)}
\label{sec:analysis}
1. Accuracy (F1 + MCC) auf SICK und eSNLI einzeln nach Kategorien
2. Auf Bias 체berpr체fen: Vergleich vom Modell f체r wichtig erachtete Token mit von Menschen als wichitg erachtete Tokens
Visualisierungen:
- Confusion Matrix (Gentrennt nach Ph채nomenen)
- Tabellen Interpretability Metriken (siehe ferret)


\printbibliography

\end{document}